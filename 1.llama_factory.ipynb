{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3084753b-5508-4dc3-afa8-9d35002a9c24",
   "metadata": {},
   "source": [
    "# LLaMA-Factory 微调大模型\n",
    "\n",
    "第一步，我们来安装 LLaMA Factory。LLaMA Factory 是一个比较简单的微调框架，支持通过 WebUI 零代码微调大语言模型。常见的微调框架还有 [unsloth](https://github.com/unslothai/unsloth), [trl](https://github.com/huggingface/trl), [peft](https://github.com/huggingface/peft)。初次微调大模型，以熟悉概念和流程为主，因此选用最好上手的 LLaMA Factory。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c524577-04bb-4781-9467-b3ed26d68c65",
   "metadata": {},
   "source": [
    "## 1. 环境准备\n",
    "\n",
    "### 1.1 安装 LLaMA Factory\n",
    "\n",
    "**1）安装 CUDA**\n",
    "\n",
    "参考：[CUDA 安装](https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/installation.html#cuda)\n",
    "\n",
    "运行以下命令检查驱动是否安装：\n",
    "\n",
    "```bash\n",
    "nvidia-smi\n",
    "```\n",
    "\n",
    "PS：假定你已经安装好驱动，驱动的安装方法这里略过。\n",
    "\n",
    "安装 CUDA Toolkit：\n",
    "\n",
    "```bash\n",
    "# 安装 build-essential 和 gcc-multilib\n",
    "sudo apt-get update\n",
    "sudo apt-get install build-essential\n",
    "sudo apt-get install gcc-multilib\n",
    "\n",
    "# 设置 CUDA 仓库\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\n",
    "sudo dpkg -i cuda-keyring_1.1-1_all.deb\n",
    "\n",
    "# 更新并安装 CUDA\n",
    "sudo apt-get update\n",
    "sudo apt-get install cuda\n",
    "```\n",
    "\n",
    "**2）安装 LLaMA-Factory**\n",
    "\n",
    "运行以下命令，安装 LLaMA-Factory：\n",
    "\n",
    "```bash\n",
    "# 创建一个文件夹放 repository\n",
    "mkdir envs && cd envs\n",
    "\n",
    "# 下载 LLaMA-Factory 代码仓库\n",
    "git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "\n",
    "# 安装 LLaMA-Factory 及其依赖\n",
    "cd LLaMA-Factory\n",
    "pip install -e \".[torch,metrics]\"\n",
    "```\n",
    "\n",
    "检查安装：\n",
    "\n",
    "```bash\n",
    "# 查看版本\n",
    "llamafactory-cli version\n",
    "\n",
    "# 查看训练相关的参数帮助\n",
    "llamafactory-cli train -h\n",
    "```\n",
    "\n",
    "检查 pytorch 环境：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(0)\n",
    "torch.__version__\n",
    "```\n",
    "\n",
    "**3）安装 bitsandbytes**\n",
    "\n",
    "如果想启用量化 LoRA（QLoRA），需要安装 bitsandbytes。\n",
    "\n",
    "在 **Linux** 上安装 bitsandbytes：\n",
    "\n",
    "```bash\n",
    "pip install bitsandbytes\n",
    "```\n",
    "\n",
    "如果想在 **Windows** 上启用 QLoRA，请根据 CUDA 版本选择合适的 [bitsandbytes](https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels/) 发行版。\n",
    "\n",
    "执行以下命令，查看 CUDA 版本:\n",
    "\n",
    "```bash\n",
    "nvcc --version\n",
    "```\n",
    "\n",
    "我的 CUDA 版本是 12.6\n",
    "\n",
    "```bash\n",
    "nvcc: NVIDIA (R) Cuda compiler driver\n",
    "Copyright (c) 2005-2024 NVIDIA Corporation\n",
    "Built on Thu_Sep_12_02:55:00_Pacific_Daylight_Time_2024\n",
    "Cuda compilation tools, release 12.6, V12.6.77\n",
    "Build cuda_12.6.r12.6/compiler.34841621_0\n",
    "```\n",
    "\n",
    "选择安装较新的版本 0.41.1：\n",
    "\n",
    "```bash\n",
    "pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06690404-c3ec-4db5-90be-b0ee0cdd3b12",
   "metadata": {},
   "source": [
    "### 1.2 下载 Qwen 模型\n",
    "\n",
    "前几个项目用的都是 Qwen 模型，秉持着推理代码可以复用的偷懒精神，这次还用 Qwen。我有 8G 显存，当前最大可接受的参数量是 7B，阿里经过指令微调的 `Qwen2.5-7B-Instruct` 模型是我最好的选择，后续开 QLoRA 可将显存消耗压制在合适范围内。\n",
    "\n",
    "多大模型用什么训练方式需要多大的 GPU 可参考：[hardware-requirement](https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file#hardware-requirement)\n",
    "\n",
    "如果显存不足，可以选择 1.5B 或 3B 模型：\n",
    "\n",
    "- [Qwen/Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)\n",
    "- [Qwen/Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)\n",
    "\n",
    "**1）安装 ModelScope**\n",
    "\n",
    "```bash\n",
    "pip install modelscope\n",
    "pip install modelscope-agent\n",
    "```\n",
    "\n",
    "**2）下载 Qwen2.5-7B-Instruct 模型**\n",
    "\n",
    "中国大陆建议使用 `ModelScope` 下载模型：\n",
    "\n",
    "```python\n",
    "from modelscope import snapshot_download\n",
    "\n",
    "model_dir = snapshot_download('Qwen/Qwen2.5-7B-Instruct', cache_dir='./')\n",
    "print(f\"model_dir: {model_dir}\")\n",
    "```\n",
    "\n",
    "> 以上代码同本仓库 [download_qwen.py](/model/download_qwen.py) 脚本。\n",
    "> \n",
    "> 可在根目录打开命令行，执行 `cd model && python download_qwen.py` 下载 Qwen 模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e0adc2-2127-4c76-b938-8dd81f7b4eb1",
   "metadata": {},
   "source": [
    "### 1.3 模型推理测试\n",
    "\n",
    "模型下载好之后，测试一下模型能否正常推理。\n",
    "\n",
    "**1）使用 transformers 库推理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e60436f-48a5-4d65-a56e-99dd0b790d3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:21:10.434655Z",
     "iopub.status.busy": "2025-04-15T08:21:10.434475Z",
     "iopub.status.idle": "2025-04-15T08:21:13.950320Z",
     "shell.execute_reply": "2025-04-15T08:21:13.949737Z",
     "shell.execute_reply.started": "2025-04-15T08:21:10.434642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "MODEL_PATH = \"./model/Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eabcf4d-49c7-4b99-a9d1-a850628c808e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:21:13.959478Z",
     "iopub.status.busy": "2025-04-15T08:21:13.959297Z",
     "iopub.status.idle": "2025-04-15T08:41:43.912426Z",
     "shell.execute_reply": "2025-04-15T08:41:43.911831Z",
     "shell.execute_reply.started": "2025-04-15T08:21:13.959464Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8d115ed8884986aad8cb03b11043ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, I be yer trusty pirate chatbot, ready to sail the cyber seas wit ye and share tales of treasure and treachery! What be yer name, matey?\n"
     ]
    }
   ],
   "source": [
    "# 模型的绝对路径\n",
    "abs_model_path = os.path.abspath(MODEL_PATH)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=abs_model_path,\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.bfloat16\n",
    "    },\n",
    "    device_map=device,  # auto cuda\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=pipeline.tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eda190-ef3a-49f8-acf3-d4228fc0590f",
   "metadata": {},
   "source": [
    "**2）使用 ChatBot 推理**\n",
    "\n",
    "LLaMA Factory 提供了一个基于 [gradio](https://github.com/gradio-app/gradio) 开发的 ChatBot 推理页面。\n",
    "\n",
    "运行以下命令启动 ChatBot：\n",
    "\n",
    "```bash\n",
    "model_path=\"./model/Qwen/Qwen2.5-7B-Instruct\"\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli webchat \\\n",
    "    --model_name_or_path $model_path\\\n",
    "    --template qwen\n",
    "```\n",
    "\n",
    "> PS: 如需了解 template 的枚举值，可查看对话模板文件：[template.py](https://github.com/hiyouga/LLaMA-Factory/blob/main/src/llamafactory/data/template.py)\n",
    "\n",
    "模型加载完毕后，浏览器打开 [http://localhost:7861/](http://localhost:7861/) 即进入推理页。\n",
    "\n",
    "![webchat_init](./img/webchat_init.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a89c4-b500-4ba3-bb28-ab2261b3cd23",
   "metadata": {},
   "source": [
    "## 2. 数据准备\n",
    "\n",
    "模型微调后用于何种任务，决定了应该使用什么数据集。业界常见的任务包括：NL2SQL，商品描述生成、广告文案生成等。但是以上任务班味太重，不适合我们 side project 的调性。经过一番挑三拣四，决定用医疗对话数据集来优化 Qwen 模型在问诊场景的效果。\n",
    "\n",
    "### 2.1 下载医疗对话数据集\n",
    "\n",
    "数据集 **shibing624/medical** 是医疗大模型 [shibing624/MedicalGPT](https://github.com/shibing624/MedicalGPT) 的训练数据集，\n",
    "\n",
    "- 从 Huggingface 下载：[shibing624/medical](https://huggingface.co/datasets/shibing624/medical/tree/main/finetune)\n",
    "- 从 Modelscope 下载：[medical_zh](https://modelscope.cn/datasets/swift/medical_zh)\n",
    "\n",
    "无需下载全部数据文件，只需要下载以下 3 个和中文 SFT 有关的数据文件即可。下载后放在本项目的 `/data` 目录：\n",
    "\n",
    "- `train_zh_0.json`\n",
    "- `test_zh_0.json`\n",
    "- `valid_zh_0.json`\n",
    "\n",
    "### 2.2 检查数据格式\n",
    "\n",
    "使用以下命令获取 `train_zh_0.json` 文件的前三行：\n",
    "\n",
    "```bash\n",
    "cd data\n",
    "head -3 train_zh_0.json\n",
    "```\n",
    "\n",
    "样例数据如下：\n",
    "\n",
    "```bash\n",
    "{\"instruction\": \"血热的临床表现是什么?\", \"input\": \"\", \"output\": \"初发或复发病不久。皮疹发展迅速，呈点滴状、钱币状或混合状。常见丘疹、斑丘疹、大小不等的斑片，潮红、鲜红或深红色。散布于体表各处或几处，以躯干、四肢多见，亦可先从头面开始，逐渐发展至全身。新皮疹不断出现，表面覆有银白色鳞屑，干燥易脱落，剥刮后有点状出血。可有同形反应;伴瘙痒、心烦口渴。大便秘结、小便短黄，舌质红赤，苔薄黄或根部黄厚，脉弦滑或滑数。血热炽盛病机，主要表现在如下四个面：一、热象：血热多属阳盛则热之实性、热性病机和病证、并表现出热象。二、血行加速：血得热则行，可使血流加速，且使脉道扩张，络脉充血，故可见面红目赤，舌色深红（即舌绛）等症。三、动血：在血行加速与脉道扩张的基础上，血分有热，可灼伤脉络，引起出血，称为“热迫血妄行”，或称动血。四、扰乱心神：血热炽盛则扰动心神，心主血脉而藏神，血脉与心相通，故血热则使心神不安，而见心烦，或躁扰发狂等症。\"}\n",
    "{\"instruction\": \"帕金森叠加综合征的辅助治疗有些什么？\", \"input\": \"\", \"output\": \"综合治疗；康复训练；生活护理指导；低频重复经颅磁刺激治疗\"}\n",
    "{\"instruction\": \"卵巢癌肉瘤的影像学检查有些什么？\", \"input\": \"\", \"output\": \"超声漏诊；声像图；MR检查；肿物超声；术前超声；CT检查\"}\n",
    "```\n",
    "\n",
    "非常棒，数据已经是 alpaca 格式，满足 LLaMA Factory 的数据格式要求，无需进行数据格式转换。\n",
    "\n",
    "### 2.3 添加描述文件\n",
    "\n",
    "使用 `wc -l train_zh_0.json` 查看文件行数，发现训练集有 194 万多行，太多了。为了快速跑完第一次微调任务，从训练集中抽取前 1000 条样本：\n",
    "\n",
    "```bash\n",
    "head -1000 train_zh_0.json > train_zh_1000.json\n",
    "```\n",
    "\n",
    "新建数据集描述文件 `data/dataset_info.json`，参考 [格式要求](https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/data_preparation.html#id4) 填写：\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"train_zh\": {\n",
    "    \"file_name\": \"train_zh_0.json\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"query\": \"input\",\n",
    "      \"response\": \"output\"\n",
    "    }\n",
    "  },\n",
    "  \"train_zh_1000\": {\n",
    "    \"file_name\": \"train_zh_1000.json\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"query\": \"input\",\n",
    "      \"response\": \"output\"\n",
    "    }\n",
    "  },\n",
    "  \"test_zh\": {\n",
    "    \"file_name\": \"test_zh_0.json\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"query\": \"input\",\n",
    "      \"response\": \"output\"\n",
    "    }\n",
    "  },\n",
    "  \"valid_zh\": {\n",
    "    \"file_name\": \"valid_zh_0.json\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"query\": \"input\",\n",
    "      \"response\": \"output\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb729b-2457-4980-92b2-16121fb5be81",
   "metadata": {},
   "source": [
    "## 3. 微调大模型\n",
    "\n",
    "以上，我们已经准备了微调框架、模型文件、微调数据集，下面开始微调模型。\n",
    "\n",
    "打开命令行，来到项目根目录，执行以下命令启动 WebUI 页面：\n",
    "\n",
    "```bash\n",
    "llamafactory-cli webui\n",
    "\n",
    "# 如需指定显卡 id 和 WebUI 端口\n",
    "# CUDA_VISIBLE_DEVICES=0 GRADIO_SERVER_PORT=7860 llamafactory-cli webui\n",
    "```\n",
    "\n",
    "### 3.1 SFT 监督微调\n",
    "\n",
    "配置好模型路径、微调方法、数据集等参数。由于我的显存限制，开启了 4-bit 量化，会损失一些训练精度。\n",
    "\n",
    "![webui_train](./img/webui_train.png)\n",
    "\n",
    "我的训练参数如下，供参考：\n",
    "\n",
    "|参数名|参数值|\n",
    "| -- | -- |\n",
    "|模型名称|Qwen2.5-7B-Instruct|\n",
    "|模型路径|model/Qwen/Qwen2___5-7B-Instruct|\n",
    "|微调方法|lora|\n",
    "|量化等级|4|\n",
    "|量化方法|bitsandbytes|\n",
    "|训练阶段|Supervised Fine-Tuning|\n",
    "|数据路径|data|\n",
    "|数据集|train_zh_1000|\n",
    "\n",
    "使用 `预览命令` 功能，生成当前训练参数下的训练脚本：\n",
    "\n",
    "```bash\n",
    "llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen/Qwen2___5-7B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset train_zh_1000 \\\n",
    "    --cutoff_len 2048 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 100000 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 100 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2.5-7B-Instruct/lora/train_2025-04-11-15-29-12 \\\n",
    "    --bf16 True \\\n",
    "    --plot_loss True \\\n",
    "    --trust_remote_code True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --optim adamw_torch \\\n",
    "    --quantization_bit 4 \\\n",
    "    --quantization_method bitsandbytes \\\n",
    "    --double_quantization True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \n",
    "```\n",
    "\n",
    "训练参数配置好后，点击“开始”进行训练。由于我们只选择了 1000 条样本进行训练，训练很快就能结束。\n",
    "\n",
    "训练中，loss 逐渐下降：\n",
    "\n",
    "![webui_training](./img/webui_training.jpg)\n",
    "\n",
    "8G 显存接近跑满：\n",
    "\n",
    "![gpu_stat](./img/gpu_stat.png)\n",
    "\n",
    "> PS: 为了防止训练因 Windows 的电源计划或睡眠设置中断，可以启用 [PowerToys](https://github.com/microsoft/PowerToys/releases/latest) 的 [唤醒功能](https://learn.microsoft.com/en-us/windows/powertoys/awake)。它支持一键维持电脑的唤醒状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84244683-82c9-47ba-a3fe-82625b59e54c",
   "metadata": {},
   "source": [
    "### 3.2 加载训练好的 LoRA 文件\n",
    "\n",
    "训练完毕后, 点击“检查点路径”，即可在下拉栏中找到该模型历史上使用 WebUI 训练的 LoRA 模型。点选我们上一次训练的检查点，后续再训练或者执行 chat 的时候，会将此 LoRA 一起加载。\n",
    "\n",
    "![webui_chat](./img/webui_chat.png)\n",
    "\n",
    "加载好之后，我们来问一个问题，看一下它回答得怎么样：`3 个月先兆流产怎么办`\n",
    "\n",
    "![webui_question](./img/webui_question.png)\n",
    "\n",
    "作为对比，下面是不加载 LoRA 的原模型的回答：\n",
    "\n",
    "![webui_question2](./img/webui_question2.png)\n",
    "\n",
    "这个问题是根据训练数据集中的这个样本提出的：\n",
    "\n",
    "```\n",
    "{\"instruction\": \"五个月先兆流产怎么办\", \"input\": \"\", \"output\": \"五个月先兆流产应该是有自然流产引起的，引起的原因可能是有胎儿停育或者有孕期感染及用药或者外伤剧烈运动等原因引起的建议，你现在先注意观察出血，最好是及时去医院做检查查看，然后再进行清宫手术的，并注意术后的休息及卫生等。怀孕五个月在家突然引起流产的情况，这个是比较危险的，导致大出血的现象，建议您需要及时的去医院妇产科就诊。流产后一定要注意休息，饮食吃一些营养丰富的食物，一个月内禁止同房，注意外阴部位的清洁卫生，避免细菌侵入，引起感染。怀孕五个月，有先兆流产，需要卧床静养，保胎药物，感觉胎动位置偏下，可以超声检查胎儿在宫内的活动情况，也要测量宫颈管长度，避免宫颈机能不全。胎儿的胎动没有规律性，有时频繁一点，也不是异常情况，大多在傍晚或者夜晚的时候感觉比较明显一些，使用胎心仪听诊胎心搏动，比较规律就没有太大的问题。若是预防入盆的可能不大，建议彩超的情况观察是否胎盘低导致的，注意少食生冷刺激性食物，建议定期到医院检查胎儿的情况保胎用药的。目前的情况是绝对卧床休息，避免劳累。放松心情，不要过于担心。注意定期产前检查。左侧卧位，有利于胎盘供氧。注意胎心及胎动情况。您需要去当地医院做B超看一下，然后在医生的指导下积极进行保胎治疗。建议您暂时尽量多卧床休息，避免过度劳累，避免剧烈运动，在家人的陪同下稳稳的去医院检查，检查过后在医生的指导下按时服药，进行保胎治疗。如果是胎儿本身问题引起的流产，需要及时终止妊娠，术后要多吃有营养的食物，禁食辛辣刺激性食物，多注意休息，避免剧烈运动。流产不用担心，半年后可以再次备孕。\"}\n",
    "```\n",
    "\n",
    "你觉得大模型的回答有几分神韵？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61fd548-c45f-49cb-aa82-2406ca80a371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T10:23:20.940083Z",
     "iopub.status.busy": "2025-04-11T10:23:20.934570Z",
     "iopub.status.idle": "2025-04-11T10:23:21.007401Z",
     "shell.execute_reply": "2025-04-11T10:23:21.004658Z",
     "shell.execute_reply.started": "2025-04-11T10:23:20.939644Z"
    }
   },
   "source": [
    "### 3.3 导出微调后的模型\n",
    "\n",
    "WebUI 支持将训练后的 LoRA 和原始大模型进行融合，输出一个完整的模型文件。\n",
    "\n",
    "导出操作也很简单。首先切换到 `Export` 分页，然后配置好导出路径和检查点路径，点击“开始导出”，少时就导出完成了。\n",
    "\n",
    "![webui_export](./img/webui_export.png)\n",
    "\n",
    "导出后的模型目录包含以下文件：\n",
    "\n",
    "```\n",
    ".\n",
    "├── Modelfile\n",
    "├── added_tokens.json\n",
    "├── config.json\n",
    "├── generation_config.json\n",
    "├── merges.txt\n",
    "├── model-00001-of-00004.safetensors\n",
    "├── model-00002-of-00004.safetensors\n",
    "├── model-00003-of-00004.safetensors\n",
    "├── model-00004-of-00004.safetensors\n",
    "├── model.safetensors.index.json\n",
    "├── special_tokens_map.json\n",
    "├── tokenizer.json\n",
    "├── tokenizer_config.json\n",
    "└── vocab.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdec4e6-6c10-47d2-95de-b87c9634f093",
   "metadata": {},
   "source": [
    "## 4. vLLM 作为推理后端\n",
    "\n",
    "### 4.1 启动后端推理服务\n",
    "\n",
    "使用 LLaMA Factory 作为推理后端，需要安装 vLLM 环境。安装过程可参考我的上一篇博客 [《本地部署大模型：Ollama 和 vLLM》](https://www.luochang.ink/posts/llm_deploy/#%E4%B8%80%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2-deepseek-r1)\n",
    "\n",
    "```bash\n",
    "# 打开安装好 vLLM 的虚拟环境\n",
    "conda activate vllm_env\n",
    "\n",
    "# 补充安装 LLaMA Factory\n",
    "cd ./envs/LLaMA-Factory/\n",
    "pip install -e \".[torch,metrics]\"\n",
    "```\n",
    "\n",
    "安装好环境后，运行下面这个脚本启动推理服务：\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# USAGE: bash server.sh\n",
    "\n",
    "source $(conda info --base)/etc/profile.d/conda.sh\n",
    "conda activate vllm_env\n",
    "\n",
    "# 我的电脑只支持运行 0.5B 模型\n",
    "# 如果你也是，请先用 ./model/download_qwen.py 下载模型\n",
    "model_path=\"./model/Qwen/Qwen2___5-0___5B-Instruct\"\n",
    "CUDA_VISIBLE_DEVICES=0 API_PORT=8621 llamafactory-cli api \\\n",
    "    --model_name_or_path $model_path \\\n",
    "    --template qwen \\\n",
    "    --infer_backend vllm \\\n",
    "    --vllm_gpu_util 0.99 \\\n",
    "    --vllm_maxlen 512 \\\n",
    "    --vllm_enforce_eager\n",
    "```\n",
    "\n",
    "### 4.2 运行客户端获取结果\n",
    "\n",
    "上一篇博客写好的 Qwen 客户端代码 [qwen_vllm_bash_client.py](https://github.com/luochang212/llm-deploy/blob/main/server/qwen_vllm_bash_client.py)，这里直接拿来用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32cac8b-db39-4def-84f8-ed04117e8b07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:17:53.298261Z",
     "iopub.status.busy": "2025-04-16T19:17:53.297046Z",
     "iopub.status.idle": "2025-04-16T19:17:53.304236Z",
     "shell.execute_reply": "2025-04-16T19:17:53.302899Z",
     "shell.execute_reply.started": "2025-04-16T19:17:53.298231Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "openai_api_base = \"http://localhost:8621/v1\"\n",
    "\n",
    "\n",
    "def chat_completion(prompt, model=''):  # llama factory 不需要写模型名，空着就行\n",
    "    client = OpenAI(\n",
    "        base_url=openai_api_base,\n",
    "        api_key='EMPTY_KEY'  # 也不需要写 api key，非空/非空字符串就行\n",
    "    )\n",
    "\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        max_tokens=512,\n",
    "        extra_body={\n",
    "            \"repetition_penalty\": 1.05,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3109e36-4ecf-4fef-94b8-25003db10871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:17:53.733030Z",
     "iopub.status.busy": "2025-04-16T19:17:53.732453Z",
     "iopub.status.idle": "2025-04-16T19:17:55.856719Z",
     "shell.execute_reply": "2025-04-16T19:17:55.856288Z",
     "shell.execute_reply.started": "2025-04-16T19:17:53.732993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抑郁症是一种常见的心理健康问题，其症状可能包括但不限于以下几点：\n",
      "\n",
      "1. **情绪低落**：持续的悲伤、空虚或快乐感丧失。\n",
      "2. **兴趣丧失**：对以前喜欢的活动失去兴趣或乐趣。\n",
      "3. **能量下降**：精力减退，无法应对日常活动。\n",
      "4. **疲劳**：持续的疲倦或缺乏精力。\n",
      "5. **睡眠障碍**：难以入睡或早醒，或者睡眠模式改变。\n",
      "6. **食欲变化**：食欲增加或减少，导致体重变化。\n",
      "7. **注意力和记忆力问题**：难以集中注意力，记忆力减退。\n",
      "8. **自杀念头或行为**：有自杀的想法或行为，可能包括企图自杀。\n",
      "9. **身体症状**：头痛、肌肉疼痛、睡眠问题、胃痛、疲劳等。\n",
      "\n",
      "如果怀疑自己或身边的人有抑郁症的症状，建议寻求专业心理医生的帮助，通过专业的评估和治疗来获得适当的帮助和支持。抑郁症是可以治疗的，通过适当的治疗和生活方式的调整，许多抑郁症患者可以恢复健康。\n"
     ]
    }
   ],
   "source": [
    "response = chat_completion(prompt=\"抑郁症有哪些症状\")\n",
    "content = response.choices[0].message.content\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fcc0d2-7a96-473d-98df-c701c81d5e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
